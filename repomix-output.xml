This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

<additional_info>

</additional_info>

</file_summary>

<directory_structure>
routers/
  agents.py
  auth.py
  organizations.py
  recordings.py
  ui_helpers.py
  users.py
  video_analyzer.py
.gitignore
main.py
models_loader.py
models.py
requirements.txt
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="routers/agents.py">
# routers/agents.py
from fastapi import APIRouter, HTTPException, Request, status
from models import Agent, AgentCreate, AgentUpdate, PyObjectId
from typing import List

router = APIRouter()

@router.post("/", response_model=Agent, status_code=status.HTTP_201_CREATED)
async def create_agent(request: Request, agent: AgentCreate):
    db = request.app.state.db
    result = await db.agents.insert_one(agent.dict())
    new_agent = await db.agents.find_one({"_id": result.inserted_id})
    return new_agent

@router.get("/", response_model=List[Agent])
async def get_agents(request: Request):
    db = request.app.state.db
    agents = await db.agents.find().to_list(100)
    return agents

@router.get("/{agent_id}", response_model=Agent)
async def get_agent(request: Request, agent_id: str):
    db = request.app.state.db
    agent = await db.agents.find_one({"_id": PyObjectId(agent_id)})
    if not agent:
        raise HTTPException(status_code=404, detail="Agent not found")
    return agent

@router.put("/{agent_id}", response_model=Agent)
async def update_agent(request: Request, agent_id: str, agent_update: AgentUpdate):
    db = request.app.state.db
    update_data = {k: v for k, v in agent_update.dict().items() if v is not None}
    result = await db.agents.update_one({"_id": PyObjectId(agent_id)}, {"$set": update_data})
    if result.modified_count == 0:
        raise HTTPException(status_code=404, detail="Agent not found or no changes made")
    agent = await db.agents.find_one({"_id": PyObjectId(agent_id)})
    return agent

@router.delete("/{agent_id}", status_code=status.HTTP_204_NO_CONTENT)
async def delete_agent(request: Request, agent_id: str):
    db = request.app.state.db
    result = await db.agents.delete_one({"_id": PyObjectId(agent_id)})
    if result.deleted_count == 0:
        raise HTTPException(status_code=404, detail="Agent not found")
    return
</file>

<file path="routers/auth.py">
# routers/auth.py
from fastapi import APIRouter, HTTPException, Request, status
from models import User, PyObjectId, pwd_context
from pydantic import BaseModel

router = APIRouter()

class LoginRequest(BaseModel):
    email: str
    password: str

@router.post("/login", response_model=User)
async def login(request: Request, credentials: LoginRequest):
    db = request.app.state.db
    user = await db.users.find_one({
        "email": credentials.email
    })
    if not user:
        raise HTTPException(status_code=401, detail="User not found. Please contact your admin.")
    if not pwd_context.verify(credentials.password, user.get("hashed_password", "")):
        raise HTTPException(status_code=401, detail="Incorrect password.")
    return user
</file>

<file path="routers/organizations.py">
# routers/organizations.py
from fastapi import APIRouter, HTTPException, Request, status
from models import Organization, OrganizationCreate, OrganizationUpdate, PyObjectId
from typing import List

router = APIRouter()

@router.post("/", response_model=Organization, status_code=status.HTTP_201_CREATED)
async def create_organization(request: Request, org: OrganizationCreate):
    db = request.app.state.db
    existing = await db.organizations.find_one({"name": org.name})
    if existing:
        raise HTTPException(status_code=400, detail="Organization already exists")
    result = await db.organizations.insert_one(org.dict())
    new_org = await db.organizations.find_one({"_id": result.inserted_id})
    return new_org

@router.get("/", response_model=List[Organization])
async def get_organizations(request: Request):
    db = request.app.state.db
    organizations = await db.organizations.find().to_list(100)
    return organizations

@router.get("/{org_id}", response_model=Organization)
async def get_organization(request: Request, org_id: str):
    db = request.app.state.db
    org = await db.organizations.find_one({"_id": PyObjectId(org_id)})
    if not org:
        raise HTTPException(status_code=404, detail="Organization not found")
    return org

@router.put("/{org_id}", response_model=Organization)
async def update_organization(request: Request, org_id: str, org_update: OrganizationUpdate):
    db = request.app.state.db
    update_data = {k: v for k, v in org_update.dict().items() if v is not None}
    result = await db.organizations.update_one({"_id": PyObjectId(org_id)}, {"$set": update_data})
    if result.modified_count == 0:
        raise HTTPException(status_code=404, detail="Organization not found or no changes made")
    org = await db.organizations.find_one({"_id": PyObjectId(org_id)})
    return org

@router.delete("/{org_id}", status_code=status.HTTP_204_NO_CONTENT)
async def delete_organization(request: Request, org_id: str):
    db = request.app.state.db
    result = await db.organizations.delete_one({"_id": PyObjectId(org_id)})
    if result.deleted_count == 0:
        raise HTTPException(status_code=404, detail="Organization not found")
    return
</file>

<file path="routers/ui_helpers.py">
# routers/ui_helpers.py
import cv2
import numpy as np
import random
from typing import Dict, List, Tuple, Any, Optional

class UIElementDetector:
    """A placeholder detector that simulates detecting UI elements in a frame."""
    def __init__(self):
        pass

    def detect(self, frame):
        # Convert to grayscale and threshold to simulate detection
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        _, thresh = cv2.threshold(gray, 200, 255, cv2.THRESH_BINARY)
        contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        ui_elements = []
        for contour in contours:
            # Skip small contours
            if cv2.contourArea(contour) < 500:
                continue
            x, y, w, h = cv2.boundingRect(contour)
            element_type = self._classify_element(w, h)
            element_text = self._extract_text(frame[y:y+h, x:x+w])
            ui_elements.append({
                'type': element_type,
                'text': element_text,
                'bbox': (x, y, w, h)
            })
        return ui_elements

    def _classify_element(self, width, height):
        aspect_ratio = width / height if height > 0 else 0
        if aspect_ratio > 5:
            return "Menu"
        elif 2 < aspect_ratio <= 5:
            return "Text field"
        elif 0.75 < aspect_ratio <= 2:
            return "Button"
        elif aspect_ratio <= 0.75:
            return "Icon"
        else:
            return "UI Element"

    def _extract_text(self, element_img):
        # In a real solution you might use OCR.
        brightness = np.mean(element_img)
        if brightness > 200:
            return "Light UI element"
        elif brightness < 100:
            return "Dark UI element"
        else:
            return "UI element"


class CursorTracker:
    """A placeholder cursor tracker that simulates cursor detection."""
    def __init__(self):
        pass

    def track(self, frame):
        # Simulate cursor detection with a 70% chance of detecting a cursor.
        if random.random() < 0.7:
            h, w = frame.shape[:2]
            x = random.randint(0, w)
            y = random.randint(0, h)
            return {
                'detected': True,
                'position': (x, y),
                'confidence': random.uniform(0.7, 0.95)
            }
        else:
            return {
                'detected': False,
                'position': None,
                'confidence': 0
            }
</file>

<file path="routers/users.py">
# routers/users.py
from fastapi import APIRouter, HTTPException, Request, status
from models import User, UserCreate, UserUpdate, PyObjectId, pwd_context
from typing import List

router = APIRouter()

@router.post("/", response_model=User, status_code=status.HTTP_201_CREATED)
async def create_user(request: Request, user: UserCreate):
    db = request.app.state.db
    # Check for duplicate email
    existing = await db.users.find_one({
        "email": user.email
    })
    if existing:
        raise HTTPException(status_code=400, detail="User already exists")
    user_data = user.dict()
    user_data["hashed_password"] = pwd_context.hash(user_data.pop("password"))
    result = await db.users.insert_one(user_data)
    new_user = await db.users.find_one({"_id": PyObjectId(result.inserted_id)})
    return new_user

@router.get("/", response_model=List[User])
async def get_users(request: Request):
    db = request.app.state.db
    users = await db.users.find().to_list(100)
    return users

@router.get("/{user_id}", response_model=User)
async def get_user(request: Request, user_id: str):
    db = request.app.state.db
    user = await db.users.find_one({"_id": PyObjectId(user_id)})
    if not user:
        raise HTTPException(status_code=404, detail="User not found")
    return user

@router.put("/{user_id}", response_model=User)
async def update_user(request: Request, user_id: str, user_update: UserUpdate):
    db = request.app.state.db
    update_data = {k: v for k, v in user_update.dict().items() if v is not None}
    if "password" in update_data:
        update_data["hashed_password"] = pwd_context.hash(update_data.pop("password"))
    result = await db.users.update_one({"_id": PyObjectId(user_id)}, {"$set": update_data})
    if result.modified_count == 0:
        raise HTTPException(status_code=404, detail="User not found or no changes made")
    user = await db.users.find_one({"_id": PyObjectId(user_id)})
    return user

@router.delete("/{user_id}", status_code=status.HTTP_204_NO_CONTENT)
async def delete_user(request: Request, user_id: str):
    db = request.app.state.db
    result = await db.users.delete_one({"_id": PyObjectId(user_id)})
    if result.deleted_count == 0:
        raise HTTPException(status_code=404, detail="User not found")
    return
</file>

<file path="routers/video_analyzer.py">
# routers/video_analyzer.py
import os
import cv2
import numpy as np
import json
import re
import logging
from typing import List, Dict, Any, Optional

from fastapi.concurrency import run_in_threadpool
from openai import OpenAI
import models_loader

logger = logging.getLogger(__name__)

class OnboardingVideoAnalyzer:
    def __init__(self, video_path: str):
        self.video_path = video_path
        self.temp_dir = "temp_processing"
        self.frames_dir = os.path.join(self.temp_dir, "frames")
        self.audio_path = os.path.join(self.temp_dir, "audio.wav")

        # Create temporary directories
        os.makedirs(self.temp_dir, exist_ok=True)
        os.makedirs(self.frames_dir, exist_ok=True)

        logger.info("Initializing OnboardingVideoAnalyzer")
        
        # Get models from loader - will raise appropriate exceptions if models aren't available
        self.transcription_model = models_loader.get_model("transcription_model")
        if self.transcription_model is None:
            raise ValueError("Transcription model not loaded. Please ensure models are loaded first.")
            
        self.nlp_summarizer = models_loader.get_model("nlp_summarizer")
        self.sentence_model = models_loader.get_model("sentence_model")
        self.ui_detector = models_loader.get_model("ui_detector")
        self.cursor_tracker = models_loader.get_model("cursor_tracker")
        
        logger.info("Analyzer initialized successfully")

    def process(self):
        """Main processing function."""
        logger.info(f"Processing video: {self.video_path}")

        # Extract audio and frames
        self.extract_audio()
        self.extract_frames()

        # Transcribe audio
        transcript = self.transcribe_audio()

        # Analyze frames (detect UI elements and cursor actions)
        frames_data = self.analyze_frames()

        # Segment into meaningful steps
        steps = self.segment_into_steps(transcript, frames_data)

        # Refine steps with GPT (if API key available)
        refined_steps = self.refine_steps_with_gpt(steps)

        return refined_steps

    def extract_audio(self):
        """Extract audio from the video file with better error handling."""
        logger.info(f"Extracting audio from {self.video_path}")
        
        try:
            # First try with moviepy
            from moviepy.editor import VideoFileClip
            try:
                video = VideoFileClip(self.video_path)
                audio = video.audio
                
                # Check if audio exists
                if audio is None:
                    logger.warning("No audio track found in video file")
                    # Create an empty audio file as fallback
                    with open(self.audio_path, 'w') as f:
                        f.write("")
                    self.video_duration = video.duration
                    self.fps = video.fps
                    video.close()
                    return
                    
                audio.write_audiofile(self.audio_path, verbose=False, logger=None)
                self.video_duration = video.duration
                self.fps = video.fps
                video.close()
                logger.info("Audio extraction complete")
                
            except Exception as moviepy_error:
                logger.warning(f"MoviePy error: {str(moviepy_error)}")
                logger.info("Trying alternative approach with FFmpeg directly...")
                
                # Alternative approach using ffmpeg directly
                import subprocess
                
                # Get video duration using ffprobe
                duration_cmd = [
                    "ffprobe", 
                    "-v", "error", 
                    "-show_entries", "format=duration", 
                    "-of", "default=noprint_wrappers=1:nokey=1", 
                    self.video_path
                ]
                
                try:
                    result = subprocess.run(duration_cmd, capture_output=True, text=True)
                    if result.returncode == 0 and result.stdout.strip():
                        self.video_duration = float(result.stdout.strip())
                    else:
                        self.video_duration = 0
                        logger.warning("Could not determine video duration, using 0")
                except Exception as e:
                    logger.warning(f"Error getting duration: {str(e)}")
                    self.video_duration = 0
                
                # Get fps using ffprobe
                fps_cmd = [
                    "ffprobe", 
                    "-v", "error", 
                    "-select_streams", "v:0", 
                    "-show_entries", "stream=r_frame_rate", 
                    "-of", "default=noprint_wrappers=1:nokey=1", 
                    self.video_path
                ]
                
                try:
                    result = subprocess.run(fps_cmd, capture_output=True, text=True)
                    if result.returncode == 0 and result.stdout.strip():
                        # Parse frame rate which might be in the format "num/den"
                        fr_parts = result.stdout.strip().split('/')
                        if len(fr_parts) == 2:
                            self.fps = float(fr_parts[0]) / float(fr_parts[1])
                        else:
                            self.fps = float(result.stdout.strip())
                    else:
                        self.fps = 30  # Default to 30 fps
                        logger.warning("Could not determine video FPS, using 30")
                except Exception as e:
                    logger.warning(f"Error getting FPS: {str(e)}")
                    self.fps = 30
                
                # Extract audio using ffmpeg
                extract_cmd = [
                    "ffmpeg",
                    "-i", self.video_path,
                    "-vn",  # No video
                    "-acodec", "pcm_s16le",  # PCM audio format
                    "-ar", "44100",  # 44.1 kHz sample rate
                    "-ac", "2",  # 2 channels (stereo)
                    "-y",  # Overwrite output file if it exists
                    self.audio_path
                ]
                
                try:
                    subprocess.run(extract_cmd, check=True, capture_output=True)
                    logger.info("Audio extraction with FFmpeg complete")
                except subprocess.CalledProcessError as e:
                    logger.error(f"FFmpeg audio extraction failed: {e.stderr.decode() if e.stderr else str(e)}")
                    
                    # Create an empty audio file as last resort
                    with open(self.audio_path, 'w') as f:
                        f.write("")
                    logger.warning("Created empty audio file as fallback")
                    
        except Exception as e:
            logger.error(f"Error extracting audio: {str(e)}")
            # Create an empty file so processing can continue
            with open(self.audio_path, 'w') as f:
                f.write("")
            logger.warning("Created empty audio file due to extraction error")
            
            # Set default values for video properties
            self.video_duration = 0
            self.fps = 30

    def extract_frames(self, frame_interval: int = 5):
        """Extract frames at regular intervals with timestamps."""
        logger.info(f"Extracting frames from {self.video_path}")
        
        try:
            cap = cv2.VideoCapture(self.video_path)
            self.frame_data = []
            frame_count = 0
            previous_frame = None

            while True:
                ret, frame = cap.read()
                if not ret:
                    break

                if frame_count % frame_interval == 0:
                    timestamp = frame_count / self.fps
                    frame_path = os.path.join(self.frames_dir, f"frame_{frame_count:06d}_{timestamp:.2f}s.jpg")
                    cv2.imwrite(frame_path, frame)
                    motion = 0
                    if previous_frame is not None:
                        motion = self.detect_motion(previous_frame, frame)
                    self.frame_data.append({
                        'frame_number': frame_count,
                        'timestamp': timestamp,
                        'path': frame_path,
                        'motion_detected': motion > 0.002  # threshold
                    })
                    previous_frame = frame.copy()
                frame_count += 1

            cap.release()
            logger.info(f"Extracted {len(self.frame_data)} frames")
            return self.frame_data
            
        except Exception as e:
            logger.error(f"Error extracting frames: {str(e)}")
            raise

    def detect_motion(self, prev_frame, curr_frame):
        """Detect motion between two frames."""
        prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)
        curr_gray = cv2.cvtColor(curr_frame, cv2.COLOR_BGR2GRAY)
        diff = cv2.absdiff(prev_gray, curr_gray)
        motion_score = np.sum(diff) / (prev_gray.shape[0] * prev_gray.shape[1] * 255)
        return motion_score

    def transcribe_audio(self):
        """Transcribe the audio using Whisper with word timestamps."""
        logger.info(f"Transcribing audio from {self.audio_path}")
        
        try:
            # Check if audio file exists and has content
            if not os.path.exists(self.audio_path) or os.path.getsize(self.audio_path) == 0:
                logger.warning("Audio file is empty or missing, returning empty transcript")
                self.transcript_segments = []
                self.transcript_text = ""
                return self.transcript_segments
                
            # Check if model is available
            if self.transcription_model is None:
                logger.error("Transcription model not available")
                self.transcript_segments = []
                self.transcript_text = ""
                return self.transcript_segments
            
            # Try transcription
            try:
                result = self.transcription_model.transcribe(
                    self.audio_path,
                    verbose=False,
                    word_timestamps=True
                )
                
                self.transcript_segments = result["segments"]
                self.transcript_text = result["text"]
                
                logger.info(f"Transcription complete: {len(self.transcript_segments)} segments")
                return self.transcript_segments
                
            except Exception as e:
                logger.error(f"Error during transcription: {str(e)}")
                self.transcript_segments = []
                self.transcript_text = ""
                return self.transcript_segments
                
        except Exception as e:
            logger.error(f"Error transcribing audio: {str(e)}")
            self.transcript_segments = []
            self.transcript_text = ""
            return self.transcript_segments
            
    def analyze_frames(self):
        """Analyze frames for UI elements, cursor position, and user interactions."""
        print("Analyzing frames for UI elements and actions...")
        analyzed_frames = []
        for i, frame_data in enumerate(self.frame_data):
            print(f"Analyzing frame {i+1}/{len(self.frame_data)}")
            frame = cv2.imread(frame_data['path'])
            ui_elements = self.ui_detector.detect(frame)
            cursor_info = self.cursor_tracker.track(frame)
            click_detected = False
            clicked_element = None
            if cursor_info['detected'] and frame_data['motion_detected']:
                for ui_elem in ui_elements:
                    x, y, w, h = ui_elem['bbox']
                    if (x <= cursor_info['position'][0] <= x + w and
                        y <= cursor_info['position'][1] <= y + h):
                        click_detected = True
                        clicked_element = ui_elem
                        break
            analyzed_frame = {
                **frame_data,
                'ui_elements': ui_elements,
                'cursor': cursor_info,
                'click_detected': click_detected,
                'clicked_element': clicked_element
            }
            analyzed_frames.append(analyzed_frame)
        return analyzed_frames
    
    def segment_into_steps(self, transcript_segments, frames_data):
        """Segment the video into logical onboarding steps using both transcript and visual cues."""
        print("Segmenting video into logical steps...")
        potential_steps = []

        # 1. Speech markers from transcript
        for segment in transcript_segments:
            text = segment['text'].lower()
            print(text)
            step_markers = ["step", "first", "second", "third", "fourth", "fifth", "next", "then", "click on", "open", "navigate"]
            for marker in step_markers:
                if marker in text:
                    potential_steps.append({
                        'type': 'speech_marker',
                        'timestamp': segment['start'],
                        'segment': segment
                    })
                    break

        # 2. UI interactions (clicks)
        for frame in frames_data:
            if frame['click_detected'] and frame['clicked_element'] is not None:
                potential_steps.append({
                    'type': 'ui_interaction',
                    'timestamp': frame['timestamp'],
                    'frame': frame
                })

        # 3. Screen transitions (change in UI elements)
        for i in range(1, len(frames_data)):
            prev_elements = len(frames_data[i-1]['ui_elements'])
            curr_elements = len(frames_data[i]['ui_elements'])
            if abs(prev_elements - curr_elements) > 3:
                potential_steps.append({
                    'type': 'screen_transition',
                    'timestamp': frames_data[i]['timestamp'],
                    'frame': frames_data[i]
                })

        potential_steps.sort(key=lambda x: x['timestamp'])
        merged_steps = []
        for step in potential_steps:
            if not merged_steps or step['timestamp'] - merged_steps[-1]['timestamp'] > 2.0:
                merged_steps.append(step)
            else:
                if step['type'] == 'speech_marker' and merged_steps[-1]['type'] != 'speech_marker':
                    merged_steps[-1] = step

        steps = []
        for i in range(len(merged_steps)):
            start_time = merged_steps[i]['timestamp']
            end_time = merged_steps[i+1]['timestamp'] if i+1 < len(merged_steps) else self.video_duration
            step_segments = [seg for seg in transcript_segments if seg['start'] >= start_time and seg['start'] < end_time]
            step_frames = [frame for frame in frames_data if frame['timestamp'] >= start_time and frame['timestamp'] < end_time]
            step_info = self.extract_step_info(step_segments, step_frames, i+1)
            steps.append(step_info)

        return steps

    def extract_step_info(self, transcript_segments, frames, step_number):
        """Extract key information for a single step."""
        step_text = " ".join([seg['text'] for seg in transcript_segments])
        interacted_elements = []
        for frame in frames:
            if frame['click_detected'] and frame['clicked_element'] is not None:
                interacted_elements.append(frame['clicked_element'])
        key_ui_elements = self.extract_key_ui_elements(frames, step_text)
        inputs = self.extract_inputs(transcript_segments, frames)
        title = self.generate_step_title(step_text, interacted_elements, key_ui_elements)
        description = self.generate_step_description(step_text, interacted_elements, key_ui_elements, inputs)
        success_criteria = self.generate_success_criteria(interacted_elements, key_ui_elements, inputs, step_text)
        return {
            'step_number': step_number,
            'title': title,
            'description': description,
            'ui_elements': key_ui_elements,
            'inputs': inputs,
            'success_criteria': success_criteria,
            'transcript': step_text,
            'interacted_elements': interacted_elements
        }

    def extract_key_ui_elements(self, frames, step_text):
        all_elements = []
        for frame in frames:
            all_elements.extend(frame['ui_elements'])
        unique_elements = {}
        for elem in all_elements:
            if elem['text'] and elem['text'] not in unique_elements:
                unique_elements[elem['text']] = elem
        scored_elements = []
        for elem_text, elem in unique_elements.items():
            mentioned = elem_text.lower() in step_text.lower()
            interacted = any(frame['click_detected'] and frame['clicked_element'] and frame['clicked_element']['text'] == elem_text for frame in frames)
            score = (1 if mentioned else 0) + (2 if interacted else 0)
            scored_elements.append({'element': elem, 'score': score})
        scored_elements.sort(key=lambda x: x['score'], reverse=True)
        top_elements = [item['element'] for item in scored_elements[:5]]
        return top_elements

    def extract_inputs(self, transcript_segments, frames):
        inputs = []
        input_patterns = [
            r'(?:type|enter|input|fill in|write)(?:\s+in)?(?:\s+the)?[:\s]+["\']?([^"\'.,;!?]+)["\']?',
            r'(?:typing|entering)(?:\s+in)?(?:\s+the)?[:\s]+["\']?([^"\'.,;!?]+)["\']?'
        ]
        transcript_text = " ".join([seg['text'] for seg in transcript_segments])
        for pattern in input_patterns:
            matches = re.finditer(pattern, transcript_text, re.IGNORECASE)
            for match in matches:
                inputs.append(match.group(1).strip())
        return inputs

    def generate_step_title(self, transcript, interacted_elements, key_ui_elements):
        action_patterns = [
            r'(?:click|tap|press)(?:\s+on)?(?:\s+the)?\s+([^\s.,;!?]+(?:\s+[^\s.,;!?]+){0,3})',
            r'(?:go|navigate)(?:\s+to)?(?:\s+the)?\s+([^\s.,;!?]+(?:\s+[^\s.,;!?]+){0,3})',
            r'(?:open|launch|start)(?:\s+up)?(?:\s+the)?\s+([^\s.,;!?]+(?:\s+[^\s.,;!?]+){0,3})',
            r'(?:select|choose)(?:\s+the)?\s+([^\s.,;!?]+(?:\s+[^\s.,;!?]+){0,3})',
            r'(?:enter|type|input)(?:\s+in)?(?:\s+the)?\s+([^\s.,;!?]+(?:\s+[^\s.,;!?]+){0,3})'
        ]
        action_types = {
            r'(?:click|tap|press)': "Click",
            r'(?:go|navigate)': "Navigate to",
            r'(?:open|launch|start)': "Open",
            r'(?:select|choose)': "Select",
            r'(?:enter|type|input)': "Enter"
        }
        for pattern in action_patterns:
            matches = re.search(pattern, transcript, re.IGNORECASE)
            if matches:
                action_object = matches.group(1)
                # Try to choose a corresponding action type
                for pattern_prefix, action_type in action_types.items():
                    if pattern.startswith(pattern_prefix):
                        return f"{action_type} {action_object}"
                return f"Interact with {action_object}"
        if interacted_elements:
            elem = interacted_elements[0]
            elem_type = elem['type'].lower()
            elem_text = elem['text']
            if elem_type == 'button':
                return f"Click the {elem_text} button"
            elif elem_type == 'link':
                return f"Navigate to {elem_text}"
            elif elem_type in ['textbox', 'input', 'text field']:
                return f"Enter information in {elem_text} field"
            else:
                return f"Interact with {elem_text}"
        if len(transcript) > 20:
            try:
                result = self.nlp_summarizer(transcript, max_length=8, min_length=3, do_sample=False)
                if result:
                    return result[0]['summary_text'].capitalize()
            except Exception as e:
                print(f"Error during summarization: {e}")
        return "Complete the step"

    def generate_step_description(self, transcript, interacted_elements, key_ui_elements, inputs):
        description = self.clean_transcript(transcript)
        if interacted_elements:
            interaction_details = []
            for elem in interacted_elements:
                elem_type = elem['type'].lower()
                elem_text = elem['text']
                if elem_type == 'button':
                    interaction_details.append(f"Click on the '{elem_text}' button.")
                elif elem_type == 'link':
                    interaction_details.append(f"Click on the '{elem_text}' link.")
                elif elem_type in ['textbox', 'input', 'text field']:
                    matching_input = None
                    for inp in inputs:
                        if (elem_text.lower() in inp.lower() or
                            any(word in elem_text.lower() for word in inp.lower().split())):
                            matching_input = inp
                            break
                    if matching_input:
                        interaction_details.append(f"Enter '{matching_input}' in the '{elem_text}' field.")
                    else:
                        interaction_details.append(f"Enter information in the '{elem_text}' field.")
                else:
                    interaction_details.append(f"Interact with the '{elem_text}' element.")
            if interaction_details and not any(detail.lower() in description.lower() for detail in interaction_details):
                description += "\n\nSpecific actions:\n- " + "\n- ".join(interaction_details)
        return description

    def clean_transcript(self, transcript):
        fillers = ['um', 'uh', 'er', 'mm', 'like', 'you know', 'sort of', 'kind of', 'basically']
        clean_text = transcript
        for filler in fillers:
            clean_text = re.sub(r'\b' + filler + r'\b', '', clean_text, flags=re.IGNORECASE)
        clean_text = re.sub(r'\s+', ' ', clean_text)
        sentences = re.split(r'(?<=[.!?])\s+', clean_text)
        sentences = [s.capitalize() for s in sentences if s]
        return ' '.join(sentences)

    def generate_success_criteria(self, interacted_elements, key_ui_elements, inputs, transcript):
        criteria = []
        for elem in interacted_elements:
            elem_type = elem['type'].lower()
            elem_text = elem['text']
            if elem_type == 'button':
                criteria.append(f"The '{elem_text}' button has been clicked")
            elif elem_type == 'link':
                criteria.append(f"The '{elem_text}' link has been clicked")
            elif elem_type in ['textbox', 'input', 'text field'] and inputs:
                criteria.append(f"Information has been entered in the '{elem_text}' field")
        success_patterns = [
            (r'you should see', "You can see"),
            (r'should appear', "The element appears on screen"),
            (r'will show', "The element is visible"),
            (r'should load', "The page has loaded"),
            (r'will take you to', "You have been redirected to the correct page")
        ]
        for pattern, result in success_patterns:
            if re.search(pattern, transcript, re.IGNORECASE):
                criteria.append(result)
        if not criteria:
            if any(elem['type'].lower() == 'button' for elem in key_ui_elements):
                criteria.append("The button has been clicked successfully")
            elif inputs:
                criteria.append("Information has been entered correctly")
            else:
                criteria.append("The step has been completed successfully")
        return criteria

    def describe_position(self, x, y, w, h, img_width: int = 1920, img_height: int = 1080):
        if x < img_width * 0.33:
            h_pos = "left"
        elif x > img_width * 0.66:
            h_pos = "right"
        else:
            h_pos = "center"
        if y < img_height * 0.33:
            v_pos = "top"
        elif y > img_height * 0.66:
            v_pos = "bottom"
        else:
            v_pos = "middle"
        if h_pos == "center" and v_pos == "middle":
            return "in the center of the screen"
        else:
            return f"in the {v_pos}-{h_pos} of the screen"

    def generate_structured_output(self, steps):
        output = ""
        for step in steps:
            output += f"## Step {step['step_number']}: {step['title']}\n\n"
            output += f"1. **Description**: {step['description']}\n\n"
            output += "2. **UI Elements to Look For**:\n"
            if step['ui_elements']:
                for elem in step['ui_elements']:
                    element_desc = f"{elem['type']}: {elem['text']}"
                    if 'bbox' in elem:
                        x, y, w, h = elem['bbox']
                        position = self.describe_position(x, y, w, h)
                        element_desc += f" ({position})"
                    output += f"   - {element_desc}\n"
            else:
                output += "   - No specific UI elements identified\n"
            output += "\n3. **Inputs/Selections Required**:\n"
            if step['inputs']:
                for input_item in step['inputs']:
                    output += f"   - {input_item}\n"
            else:
                output += "   - No specific inputs required\n"
            output += "\n4. **Success Criteria**:\n"
            for criteria in step['success_criteria']:
                output += f"   - {criteria}\n"
            output += "\n---\n\n"
        return output

    def process_openai_response_to_json(self, openai_response):
        """
        Processes the raw text response from OpenAI and converts it to JSON.
        Handles errors gracefully.
        
        Args:
            openai_response (str): The raw text response from OpenAI.
            
        Returns:
            dict or list: The parsed JSON data if successful, or None if parsing fails.
        """
        logger.info("Processing OpenAI response to JSON")
        
        if not openai_response:
            logger.warning("Empty response received, returning None")
            return None
        
        # Try to extract JSON if response contains markdown code blocks
        if "```json" in openai_response:
            try:
                json_content = openai_response.split("```json")[1].split("```")[0].strip()
                parsed_data = json.loads(json_content)
                logger.info(f"Successfully parsed JSON from markdown code block")
                return parsed_data
            except (IndexError, json.JSONDecodeError) as e:
                logger.error(f"Failed to extract JSON from markdown: {str(e)}")
                # Continue to try parsing the whole response
        
        # Try to parse the entire response as JSON
        try:
            parsed_data = json.loads(openai_response)
            logger.info(f"Successfully parsed JSON from response")
            return parsed_data
        except json.JSONDecodeError as e:
            logger.error(f"Failed to parse JSON from response: {str(e)}")
            
            # Last resort: try to find anything that looks like JSON in the response
            try:
                # Look for content between curly braces
                import re
                json_pattern = r'(\{.*\}|\[.*\])'
                matches = re.search(json_pattern, openai_response, re.DOTALL)
                if matches:
                    potential_json = matches.group(0)
                    parsed_data = json.loads(potential_json)
                    logger.warning(f"Extracted JSON using regex pattern")
                    return parsed_data
            except (re.error, json.JSONDecodeError) as e:
                logger.error(f"All JSON parsing attempts failed: {str(e)}")
        
        logger.error("Could not convert response to JSON")
        return None

    def refine_steps_with_gpt(self, steps):
        """
        Uses OpenAI GPT-4 (or equivalent) to refine and structure the extracted steps.
        Handles missing API key gracefully.
        """
        logger.info("Refining steps with GPT")
        
        # If no steps or API key, return the steps as is
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key or not steps:
            logger.warning("No API key or steps, skipping GPT refinement")
            return steps
        
        # Generate structured output for prompt
        structured_output = self.generate_structured_output(steps)
        
        prompt = (
            "Extract the onboarding steps from the following data. For each step, provide:\n"
            "1. A clear title summarizing the step\n"
            "2. A detailed description explaining exactly what to do\n"
            "3. Any UI elements to look for (buttons, fields, menus)\n"
            "4. Any specific inputs or selections that need to be made\n"
            "5. Success criteria for completing the step\n\n"
            "Here is the extracted data:\n\n"
            f"{structured_output}\n\n"
            "Format your response as a valid JSON array of step objects."
        )
        
        try:
            client = OpenAI(api_key=api_key)
            response = client.chat.completions.create(
                model="gpt-4o-mini",  # change to your desired model
                messages=[
                    {"role": "system", "content": "You are an assistant that extracts structured onboarding steps in JSON data."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.2,
            )
            raw_output = response.choices[0].message.content
            
            # Process the raw output to JSON using the new function
            refined_steps = self.process_openai_response_to_json(raw_output)
            
            if refined_steps:
                logger.info(f"Successfully refined steps with GPT: {len(refined_steps)} steps")
                print(f"Steps are: {refined_steps}")
                return refined_steps
            else:
                logger.warning("Could not parse GPT output to JSON, returning original steps")
                return steps
                
        except Exception as e:
            logger.error(f"Error calling OpenAI API: {str(e)}")
            return steps
</file>

<file path="models_loader.py">
# models_loader.py
import os
import ssl
import logging
from typing import Dict, Any, Optional
import time
import certifi

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Fix SSL certificate issues
ssl_context = ssl.create_default_context(cafile=certifi.where())
ssl._create_default_https_context = ssl._create_unverified_context  # This bypasses certificate verification

# Global variables to store models
models = {
    "transcription_model": None,
    "nlp_summarizer": None,
    "sentence_model": None,
    "ui_detector": None,
    "cursor_tracker": None
}

model_status = {
    "loaded": False,
    "loading": False,
    "error": None
}

async def load_models_async():
    """
    Load ML models asynchronously for video processing.
    Uses deferred imports to prevent startup crashes.
    """
    global models, model_status
    
    # If already loading or loaded successfully, don't do it again
    if model_status["loading"]:
        logger.info("Models already loading, returning")
        return models
        
    if model_status["loaded"] and not model_status["error"]:
        logger.info("Models already loaded, returning")
        return models
        
    model_status["loading"] = True
    
    try:
        # Import heavy libraries only when needed
        from fastapi.concurrency import run_in_threadpool
        
        # First load lightweight models that won't crash the app
        logger.info("Loading lightweight components first...")
        
        # UI Element Detector (lightweight)
        from routers.ui_helpers import UIElementDetector, CursorTracker
        models["ui_detector"] = UIElementDetector()
        logger.info("✓ Loaded UI detector")
        
        models["cursor_tracker"] = CursorTracker()
        logger.info("✓ Loaded cursor tracker")
        
        # Load whisper with exception handling and threadpool
        logger.info("Loading whisper model (tiny)...")
        try:
            import whisper
            # Wrap in threadpool to prevent blocking
            models["transcription_model"] = await run_in_threadpool(
                lambda: whisper.load_model("tiny")
            )
            logger.info("✓ Loaded whisper model")
        except Exception as e:
            logger.error(f"Failed to load whisper model: {e}")
            model_status["error"] = f"Whisper error: {str(e)}"
        
        # Optional models - can fail gracefully
        # try:
        #     logger.info("Loading NLP models (may take a moment)...")
        #     from transformers import pipeline
        #     # Use a lightweight model variant
        #     models["nlp_summarizer"] = pipeline(
        #         "summarization", 
        #         model="facebook/bart-large-cnn", 
        #         device=-1  # Use CPU to prevent CUDA issues
        #     )
        #     logger.info("✓ Loaded NLP models")
        # except Exception as e:
        #     logger.warning(f"NLP models not available: {e}")
        
        # try:
        #     logger.info("Loading sentence transformer...")
        #     from sentence_transformers import SentenceTransformer
        #     models["sentence_model"] = SentenceTransformer('all-MiniLM-L6-v2')
        #     logger.info("✓ Loaded sentence transformer")
        # except Exception as e:
        #     logger.warning(f"Sentence transformer not available: {e}")
        
        # Mark as loaded even if some optional models failed
        model_status["loaded"] = True
        model_status["loading"] = False
        logger.info("Model loading complete")
        
    except Exception as e:
        # Catch any unexpected errors during load
        logger.error(f"Unexpected error loading models: {e}")
        model_status["error"] = str(e)
        model_status["loading"] = False
    
    return models

def get_model(name: str) -> Any:
    """
    Safely get a model by name, with better error handling.
    
    Args:
        name: Name of the model to retrieve
        
    Returns:
        The requested model or None if not available
    """
    if name not in models:
        logger.warning(f"Requested unknown model: {name}")
        return None
        
    if models[name] is None:
        logger.warning(f"Model {name} not loaded yet")
        
    return models[name]

def get_model_status() -> Dict[str, Any]:
    """Get the current status of model loading"""
    return {
        "loaded": model_status["loaded"],
        "loading": model_status["loading"],
        "error": model_status["error"],
        "available_models": [name for name, model in models.items() if model is not None]
    }
</file>

<file path=".gitignore">
.DS_Store
.env
__pycache__/
repomix-output.txt
temp_processing/
</file>

<file path="models.py">
# models.py
from pydantic import BaseModel, EmailStr, Field
from typing import List, Optional
from bson import ObjectId
from passlib.context import CryptContext

# Password hashing context
pwd_context = CryptContext(schemes=["bcrypt"], deprecated="auto")

# Helper to validate ObjectId fields
class PyObjectId(ObjectId):
    @classmethod
    def __get_validators__(cls):
        yield cls.validate

    @classmethod
    def validate(cls, v, info):
        if not ObjectId.is_valid(v):
            raise ValueError("Invalid ObjectId")
        return ObjectId(v)

    @classmethod
    def __get_pydantic_json_schema__(cls, core_schema, handler):
        return {"type": "string"}

# ===== Organization models removed =====

# User Models
class UserBase(BaseModel):
    email: EmailStr
    role: str  # "Admin" or "Employee"

class UserCreate(UserBase):
    # When a user is created from the dashboard, default password is "12345678"
    password: str = "12345678"

class UserUpdate(BaseModel):
    email: Optional[EmailStr]
    role: Optional[str]
    password: Optional[str]

class User(UserBase):
    id: PyObjectId = Field(default_factory=PyObjectId, alias="_id")
    hashed_password: str

    class Config:
        allow_population_by_field_name = True
        json_encoders = {ObjectId: str}

# Agent Models
class AgentBase(BaseModel):
    name: str
    role: str  # e.g. "Software Engineer"
    description: Optional[str] = None

class OnboardingStep(BaseModel):
    """Model for an onboarding step"""
    title: str
    description: str
    recordingUrl: Optional[str] = None
    _id: Optional[str] = None

class StepsUpdate(BaseModel):
    """Model for updating agent steps"""
    steps: List[OnboardingStep]

class AgentCreate(AgentBase):
    emails: List[EmailStr] = []  # Authorized employee emails
    steps: List[OnboardingStep]  # Onboarding steps

class AgentUpdate(BaseModel):
    name: Optional[str]
    role: Optional[str]
    description: Optional[str]
    emails: Optional[List[EmailStr]]
    steps: Optional[List[dict]]

class Agent(AgentBase):
    id: PyObjectId = Field(default_factory=PyObjectId, alias="_id")
    emails: List[EmailStr] = []
    steps: List[dict] = []
    
    class Config:
        allow_population_by_field_name = True
        json_encoders = {ObjectId: str}
</file>

<file path="routers/recordings.py">
# routers/recordings.py
import os
import logging
import json
import re
import tempfile
from typing import List, Optional, Dict, Any
from bson import ObjectId

from fastapi import APIRouter, UploadFile, File, HTTPException, Request, Form, status, BackgroundTasks
from fastapi.responses import StreamingResponse, JSONResponse
from fastapi.concurrency import run_in_threadpool

import models_loader
from routers.video_analyzer import OnboardingVideoAnalyzer

router = APIRouter()
logger = logging.getLogger(__name__)

# Add an endpoints to check/initiate model loading
@router.get("/status")
async def model_status():
    """Check the status of ML models loading"""
    return models_loader.get_model_status()

@router.post("/load-models")
async def load_models():
    """Manually trigger model loading"""
    models = await models_loader.load_models_async()
    return {
        "status": "Models loading initiated/refreshed",
        "model_status": models_loader.get_model_status()
    }

# Process a recording and extract steps
async def process_recording(file_path: str, file_id: str, db):
    """Process recording to extract onboarding steps and update the database."""
    try:
        logger.info(f"Processing recording {file_id} at {file_path}")

        # Check if we have the necessary models
        if models_loader.get_model("transcription_model") is None:
            raise ValueError("Transcription model not available")
            
        # Extract steps from the video
        logger.info("Starting extraction process")
        steps = await extract_steps_from_recording(file_path)
        logger.info(f"Extracted {len(steps) if isinstance(steps, list) else 'unknown'} steps")

        # Update metadata in GridFS
        await db.fs.files.update_one(
            {"_id": ObjectId(file_id)},
            {"$set": {
                "metadata.extracted_steps": steps,
                "metadata.processing_status": "complete"
            }}
        )
        
        # Clean up the temporary video file
        try:
            os.remove(file_path)
            logger.info(f"Deleted temporary file {file_path}")
        except Exception as e:
            logger.error(f"Error cleaning up video file: {e}")
            
    except Exception as e:
        logger.error(f"Error processing recording: {str(e)}")
        # Update metadata to indicate processing failed
        await db.fs.files.update_one(
            {"_id": ObjectId(file_id)},
            {"$set": {"metadata.processing_error": str(e), "metadata.processing_status": "failed"}}
        )
        raise

# Extract steps from a video file
async def extract_steps_from_recording(video_path: str):
    """
    Asynchronously extract steps from a recording.
    Uses threadpool to avoid blocking the event loop.
    """
    try:
        logger.info(f"Creating analyzer for {video_path}")
        analyzer = OnboardingVideoAnalyzer(video_path)
        logger.info("Starting analysis process")
        result = await run_in_threadpool(analyzer.process)
        logger.info("Analysis complete")
        return result
    except Exception as e:
        logger.error(f"Error in extraction: {str(e)}")
        raise

@router.post("/upload", status_code=status.HTTP_201_CREATED)
async def upload_recording(
    request: Request,
    background_tasks: BackgroundTasks,
    file: UploadFile = File(...),
    step_index: str = Form(...)
):
    """Upload a recording file to MongoDB GridFS and process it with AI."""
    if not file.content_type or not file.content_type.startswith("video/"):
        raise HTTPException(
            status_code=400,
            detail="Invalid file type. Only video files are accepted."
        )
    
    try:
        db = request.app.state.db
        from motor.motor_asyncio import AsyncIOMotorGridFSBucket
        fs = AsyncIOMotorGridFSBucket(db)
        
        # Generate a unique filename
        filename = f"{os.urandom(8).hex()}-step{step_index}.webm"
        
        # Create a temporary file to store the video
        with tempfile.NamedTemporaryFile(suffix=".webm", delete=False) as temp_file:
            temp_file.write(await file.read())
            temp_file_path = temp_file.name
        
        # Store metadata
        metadata = {
            "content_type": file.content_type,
            "step_index": int(step_index),
            "processing_status": "pending"
        }
        
        # Upload to GridFS
        with open(temp_file_path, "rb") as f_in:
            file_bytes = f_in.read()
        file_id = await fs.upload_from_stream(
            filename,
            file_bytes,
            metadata=metadata
        )
        
        # Schedule background processing
        background_tasks.add_task(
            process_recording,
            temp_file_path,
            str(file_id),
            db
        )
        
        # Create the URL for accessing the file
        file_url = f"/recordings/{file_id}"
        
        return {
            "url": file_url,
            "file_id": str(file_id),
            "processing_status": "pending"
        }
    except Exception as e:
        logger.error(f"Error uploading recording: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Upload failed: {str(e)}")

@router.get("/{file_id}")
async def get_recording(request: Request, file_id: str):
    """Retrieve a recording file from GridFS by its ID."""
    try:
        db = request.app.state.db
        from motor.motor_asyncio import AsyncIOMotorGridFSBucket
        fs = AsyncIOMotorGridFSBucket(db)
        
        file_info = await db.fs.files.find_one({"_id": ObjectId(file_id)})
        if not file_info:
            raise HTTPException(status_code=404, detail="File not found")
        
        metadata = file_info.get("metadata", {})
        content_type = metadata.get("content_type", "video/webm")
        
        grid_out = await fs.open_download_stream(ObjectId(file_id))
        
        return StreamingResponse(
            grid_out,
            media_type=content_type,
            headers={"Content-Disposition": f"inline; filename={file_info.get('filename')}"}
        )
    except Exception as e:
        logger.error(f"Error retrieving file: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error retrieving file: {str(e)}")

@router.get("/{file_id}/metadata")
async def get_recording_metadata(request: Request, file_id: str):
    """Get metadata and extracted steps for a recording."""
    try:
        db = request.app.state.db
        file_info = await db.fs.files.find_one({"_id": ObjectId(file_id)})
        if not file_info:
            raise HTTPException(status_code=404, detail="File not found")
        
        metadata = file_info.get("metadata", {})
        return {
            "file_id": file_id,
            "processing_status": metadata.get("processing_status", "unknown"),
            "step_index": metadata.get("step_index"),
            "extracted_steps": metadata.get("extracted_steps", []),
            "error": metadata.get("processing_error")
        }
    except Exception as e:
        logger.error(f"Error retrieving metadata: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error retrieving metadata: {str(e)}")

@router.delete("/{file_id}", status_code=status.HTTP_204_NO_CONTENT)
async def delete_recording(request: Request, file_id: str):
    """Delete a recording file from GridFS."""
    try:
        db = request.app.state.db
        from motor.motor_asyncio import AsyncIOMotorGridFSBucket
        fs = AsyncIOMotorGridFSBucket(db)
        await fs.delete(ObjectId(file_id))
    except Exception as e:
        logger.error(f"Error deleting file: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error deleting file: {str(e)}")
</file>

<file path="main.py">
# main.py
import os
import logging
from fastapi import FastAPI
from motor.motor_asyncio import AsyncIOMotorClient
from routers import users, agents, auth, recordings
from dotenv import load_dotenv
from fastapi.middleware.cors import CORSMiddleware
import certifi
from contextlib import asynccontextmanager
import models_loader
import ssl

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Fix SSL verification issues
ssl._create_default_https_context = ssl._create_unverified_context

# Load environment variables
load_dotenv()

@asynccontextmanager
async def lifespan(app: FastAPI):
    # Startup
    logger.info("Starting application...")
    
    # Load models at startup
    logger.info("Loading ML models at startup...")
    try:
        await models_loader.load_models_async()
        logger.info("Models loaded successfully at startup")
    except Exception as e:
        logger.error(f"Error loading models at startup: {e}")
        logger.info("Application will continue without some models")
    
    yield
    
    # Shutdown
    logger.info("Shutting down application...")

# Create FastAPI app with proper lifespan
app = FastAPI(
    title="Iro Onboarding Backend",
    description="API for onboarding process management",
    lifespan=lifespan
)

# MongoDB connection
MONGODB_URL = os.getenv("MONGODB_URL", "mongodb://localhost:27017")
logger.info(f"Connecting to MongoDB at {MONGODB_URL}...")

try:
    client = AsyncIOMotorClient(
        MONGODB_URL,
        tlsCAFile=certifi.where(),
    )
    # Verify connection works
    client.admin.command('ping')
    logger.info("MongoDB connection successful")
    
    db = client["iro"]
    # Attach the DB to the app state
    app.state.db = db
except Exception as e:
    logger.error(f"MongoDB connection failed: {e}")
    # Continue anyway - some routes might not need the database

# Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Change for production
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Health check endpoint
@app.get("/health")
async def health_check():
    model_status = models_loader.get_model_status()
    return {
        "status": "healthy",
        "models": model_status
    }

# Include routers
app.include_router(users.router, prefix="/users", tags=["Users"])
app.include_router(agents.router, prefix="/agents", tags=["Agents"])
app.include_router(auth.router, prefix="/auth", tags=["Auth"])
app.include_router(recordings.router, prefix="/recordings", tags=["Recordings"])

if __name__ == "__main__":
    import uvicorn
    logger.info("Starting FastAPI server...")
    try:
        uvicorn.run(
            "main:app", 
            host="0.0.0.0", 
            port=8000, 
            reload=True,
            log_level="info",
            ssl_keyfile=None,  # Disable SSL for local development
            ssl_certfile=None  # Disable SSL for local development
        )
    except OSError as e:
        if "Address already in use" in str(e):
            logger.error(f"Port 8000 is already in use. Try killing the existing process.")
            # Suggest a different port
            import socket
            s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            s.bind(('', 0))
            available_port = s.getsockname()[1]
            s.close()
            logger.info(f"Try using port {available_port} instead by running:")
            logger.info(f"python -m uvicorn main:app --host 0.0.0.0 --port {available_port}")
        else:
            logger.error(f"Error starting server: {e}")
</file>

<file path="requirements.txt">
fastapi>=0.104.0
uvicorn>=0.23.2
motor>=3.3.1
python-dotenv>=1.0.0
pydantic>=2.4.2
passlib[bcrypt]>=1.7.4
bson>=0.5.10
python-multipart>=0.0.6
httpx>=0.25.0
certifi>=2023.7.22
transformers>=4.34.0
torch>=2.1.0
openai>=1.1.1
whisper>=1.1.10
opencv-python-headless>=4.8.1.78
moviepy>=1.0.3
numpy>=1.24.3
sentence-transformers>=2.2.2
</file>

</files>
